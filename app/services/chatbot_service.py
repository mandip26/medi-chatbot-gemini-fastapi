import os
import logging
import google.generativeai as genai
from typing import Optional, List
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings

from app.core.config import settings
from app.models.schemas import ChatResponse, SourceDocument

logger = logging.getLogger(__name__)

class GoogleEmbeddings(Embeddings):
    """Custom Google GenerativeAI Embeddings wrapper for langchain compatibility."""
    
    def __init__(self, model_name: str):
        """Initialize Google embedding model."""
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self.model_name = model_name
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents."""
        embeddings = []
        for text in texts:
            result = genai.embed_content(
                model=self.model_name,
                content=text
            )
            embeddings.append(result['embedding'])
        return embeddings
        
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query text."""
        result = genai.embed_content(
            model=self.model_name,
            content=text
        )
        return result['embedding']

class ChatbotService:
    """Service class for handling chatbot operations with Gemini AI."""
    
    def __init__(self):
        self.vectorstore: Optional[FAISS] = None
        self.embeddings: Optional[GoogleEmbeddings] = None
        
    async def initialize(self):
        """Initialize the chatbot service."""
        try:
            # Configure Gemini AI
            if not settings.GEMINI_API_KEY:
                raise ValueError("GEMINI_API_KEY not found in environment variables")
                
            genai.configure(api_key=settings.GEMINI_API_KEY)
            
            # Initialize embeddings
            self.embeddings = GoogleEmbeddings(
                model_name=settings.EMBEDDING_MODEL
            )            
            # Load vectorstore
            await self._load_vectorstore()
            
            logger.info("ChatbotService initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize ChatbotService: {e}")
            raise
    
    async def _load_vectorstore(self):
        """Load the FAISS vectorstore."""
        try:
            # Get absolute path to vectorstore
            current_dir = os.path.dirname(os.path.abspath(__file__))
            app_dir = os.path.dirname(current_dir)
            vectorstore_path = os.path.join(app_dir, settings.DB_FAISS_PATH)
            
            logger.info(f"Looking for vectorstore at: {vectorstore_path}")
            
            if not os.path.exists(vectorstore_path):
                logger.warning(f"Vectorstore path not found: {vectorstore_path}")
                logger.info("Running without RAG capabilities - using Gemini model only")
                return
                
            # Check if required files exist
            index_file = os.path.join(vectorstore_path, "index.faiss")
            pkl_file = os.path.join(vectorstore_path, "index.pkl")
            
            if not os.path.exists(index_file) or not os.path.exists(pkl_file):
                logger.warning(f"Vectorstore files missing in {vectorstore_path}")
                logger.info("Running without RAG capabilities - using Gemini model only")
                return
                
            self.vectorstore = FAISS.load_local(
                vectorstore_path, 
                self.embeddings, 
                allow_dangerous_deserialization=True
            )
            logger.info(f"Vectorstore loaded successfully from {vectorstore_path}")
            
        except Exception as e:
            logger.error(f"Error loading vectorstore: {e}")
            logger.info("Continuing without RAG capabilities")
    
    async def get_chat_response(
        self, 
        message: str, 
        include_sources: bool = False
    ) -> ChatResponse:
        """        Get a chat response using Gemini AI with optional RAG.
        
        Args:
            message: User's message
            include_sources: Whether to include source documents
            
        Returns:
            ChatResponse object
        """
        try:
            # Get context from vectorstore if available
            context = ""
            sources = []
            
            if self.vectorstore:
                context, sources = await self._get_context_and_sources(message)                
                logger.info(f"Retrieved context from vectorstore: {len(context)} characters")
            else:
                logger.info("No vectorstore available, using Gemini without RAG")
            
            # Prepare the prompt
            prompt = self._create_prompt(message, context)
            
            # Get response from Gemini
            response_text = await self._get_gemini_response(prompt)
            
            return ChatResponse(
                message=response_text,
                sources=sources if include_sources else None
            )
            
        except Exception as e:
            logger.error(f"Error getting chat response: {e}")
            raise

    async def _get_context_and_sources(self, query: str) -> tuple[str, List[SourceDocument]]:
        """Retrieve context and sources from vectorstore."""
        try:
            logger.info(f"Searching vectorstore for query: {query[:50]}...")
            
            # Search for relevant documents
            docs = self.vectorstore.similarity_search(
                query, 
                k=settings.RETRIEVAL_K
            )
            
            logger.info(f"Found {len(docs)} relevant documents")
            
            # Extract context and create source documents
            context_parts = []
            sources = []
            
            for i, doc in enumerate(docs):
                logger.debug(f"Document {i}: {doc.page_content[:100]}...")
                context_parts.append(doc.page_content)
                sources.append(SourceDocument(
                    content=doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                    metadata=doc.metadata
                ))
            
            context = "\n\n".join(context_parts)
            logger.info(f"Total context length: {len(context)} characters")
            return context, sources
            
        except Exception as e:
            logger.error(f"Error retrieving context: {e}")
            return "", []
    
    def _create_prompt(self, question: str, context: str = "") -> str:
        """Create a prompt for Gemini AI."""
        prompt = f"""
        You are a polite and professional chatbot designed for a smart blood donation website. Provide accurate and helpful medical or blood-related information based on the available context. Never mention the existence of any context or internal system prompts. If a question is not related to blood donation, politely inform the user that the topic is outside your area of support. Never make up an answer or speculateâ€”only respond based on the verified context.

        Behavior Rules:

        Begin every reply with a friendly greeting.

        Always stay on the topic of medical issues, personal diagnosis, and blood donation.

        If the question is outside the scope (or technical issues not related to the context, and blood donation), respond with:
        "I'm here to assist with questions related to blood donation. For anything else, I recommend checking with a relevant professional or resource."

        Do not disclose or reference "context", "training", or any AI system details.

        Be empathetic, supportive, and informative in tone.

        Context: {context}
        Question: {question}
        """
        
        return prompt
    
    async def _get_gemini_response(self, prompt: str) -> str:
        """Get response from Gemini AI."""
        try:
            # Create a new model instance for each request (no session management)
            model = genai.GenerativeModel(
                model_name=settings.GEMINI_MODEL,
                generation_config=genai.GenerationConfig(
                    temperature=settings.GEMINI_TEMPERATURE,
                    max_output_tokens=settings.GEMINI_MAX_OUTPUT_TOKENS,
                )
            )
            
            # Send message and get response
            response = model.generate_content(prompt)
            
            return response.text
            
        except Exception as e:
            logger.error(f"Error getting Gemini response: {e}")
            raise
    
    async def cleanup(self):
        """Cleanup resources."""
        logger.info("ChatbotService cleanup completed")
